@article{Reference1,
	Abstract = {We have developed an enhanced Littrow configuration extended cavity diode laser (ECDL) that can be tuned without changing the direction of the output beam. The output of a conventional Littrow ECDL is reflected from a plane mirror fixed parallel to the tuning diffraction grating. Using a free-space Michelson wavemeter to measure the laser wavelength, we can tune the laser over a range greater than 10 nm without any alteration of alignment.},
	Author = {C. J. Hawthorn and K. P. Weber and R. E. Scholten},
	Journal = {Review of Scientific Instruments},
	Month = {December},
	Number = {12},
	Numpages = {3},
	Pages = {4477--4479},
	Title = {Littrow Configuration Tunable External Cavity Diode Laser with Fixed Direction Output Beam},
	Volume = {72},
	Url = {http://link.aip.org/link/?RSI/72/4477/1},
	Year = {2001}}

@article{Reference3,
	Abstract = {Operating a laser diode in an extended cavity which provides frequency-selective feedback is a very effective method of reducing the laser's linewidth and improving its tunability. We have developed an extremely simple laser of this type, built from inexpensive commercial components with only a few minor modifications. A 780~nm laser built to this design has an output power of 80~mW, a linewidth of 350~kHz, and it has been continuously locked to a Doppler-free rubidium transition for several days.},
	Author = {A. S. Arnold and J. S. Wilson and M. G. Boshier},
	Journal = {Review of Scientific Instruments},
	Month = {March},
	Number = {3},
	Numpages = {4},
	Pages = {1236--1239},
	Title = {A Simple Extended-Cavity Diode Laser},
	Volume = {69},
	Url = {http://link.aip.org/link/?RSI/69/1236/1},
	Year = {1998}}

@article{Reference2,
	Abstract = {We present a review of the use of diode lasers in atomic physics with an extensive list of references. We discuss the relevant characteristics of diode lasers and explain how to purchase and use them. We also review the various techniques that have been used to control and narrow the spectral outputs of diode lasers. Finally we present a number of examples illustrating the use of diode lasers in atomic physics experiments. Review of Scientific Instruments is copyrighted by The American Institute of Physics.},
	Author = {Carl E. Wieman and Leo Hollberg},
	Journal = {Review of Scientific Instruments},
	Keywords = {Diode Laser},
	Month = {January},
	Number = {1},
	Numpages = {20},
	Pages = {1--20},
	Title = {Using Diode Lasers for Atomic Physics},
	Volume = {62},
	Url = {http://link.aip.org/link/?RSI/62/1/1},
	Year = {1991}}

@inproceedings{vaswani_2017_attention,
	Author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {5998--6008},
	Title = {Attention is All You Need},
	Volume = {30},
	Year = {2017}}

@article{brown_2020_language,
	Author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and others},
	Journal = {arXiv preprint arXiv:2005.14165},
	Title = {Language Models are Few-Shot Learners},
	Year = {2020}}

@article{touvron_2023_llama,
	Author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and others},
	Journal = {arXiv preprint arXiv:2302.13971},
	Title = {LLaMA: Open and Efficient Foundation Language Models},
	Year = {2023}}

@article{geva_2021_transformer,
	Author = {Mor Geva and Roei Schuster and Jonathan Berant and Omer Levy},
	Journal = {Transactions of the Association for Computational Linguistics},
	Pages = {652--669},
	Title = {Transformer Feed-Forward Layers Are Key-Value Memories},
	Volume = {9},
	Year = {2021}}

@inproceedings{meng_2022_locating,
	Author = {Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {17359--17372},
	Title = {Locating and Editing Factual Associations in GPT},
	Volume = {35},
	Year = {2022}}

@article{rogers_2020_primer,
	Author = {Anna Rogers and Olga Kovaleva and Anna Rumshisky},
	Journal = {Transactions of the Association for Computational Linguistics},
	Pages = {398--430},
	Title = {A Primer in BERTology: What We Know About How BERT Works},
	Volume = {8},
	Year = {2020}}

@article{mccloskey_1989_catastrophic,
	Author = {Michael McCloskey and Neal J. Cohen},
	Journal = {Psychology of Learning and Motivation},
	Pages = {109--165},
	Publisher = {Academic Press},
	Title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
	Volume = {24},
	Year = {1989}}

@article{kirkpatrick_2017_overcoming,
	Author = {James Kirkpatrick and Razvan Pascanu and Neil Rabinowitz and Joel Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tomas Ramalho and Agnieszka Grabska-Barwinska and others},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {13},
	Pages = {3521--3526},
	Publisher = {National Acad Sciences},
	Title = {Overcoming Catastrophic Forgetting in Neural Networks},
	Volume = {114},
	Year = {2017}}

@inproceedings{mitchell_2022_mend,
	Author = {Eric Mitchell and Charles Lin and Antoine Bosselut and Chelsea Finn and Christopher D. Manning},
	Booktitle = {International Conference on Learning Representations},
	Title = {Fast Model Editing at Scale},
	Year = {2022}}

@inproceedings{hu_2022_lora,
	Author = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	Booktitle = {International Conference on Learning Representations},
	Title = {LoRA: Low-Rank Adaptation of Large Language Models},
	Year = {2022}}

@inproceedings{meng_2022_memit,
	Author = {Kevin Meng and Arnab Sen Sharma and Alex Andonian and Yonatan Belinkov and David Bau},
	Booktitle = {International Conference on Learning Representations},
	Title = {Mass-Editing Memory in a Transformer},
	Year = {2023}}

@inproceedings{mitchell_2022_rome,
	Author = {Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {17359--17372},
	Title = {Locating and Editing Factual Associations in GPT},
	Volume = {35},
	Year = {2022}}

@article{petroni_2019_language,
	Author = {Fabio Petroni and Tim Rockt{\"a}schel and Sebastian Riedel and Patrick Lewis and Anton Bakhtin and Yuxiang Wu and Alexander Miller},
	Journal = {Transactions of the Association for Computational Linguistics},
	Pages = {61--76},
	Title = {Language Models as Knowledge Bases?},
	Volume = {7},
	Year = {2019}}

@article{jiang_2020_how,
	Author = {Zhengbao Jiang and Frank F. Xu and Jun Araki and Graham Neubig},
	Journal = {Transactions of the Association for Computational Linguistics},
	Pages = {422--438},
	Title = {How Can We Know What Language Models Know?},
	Volume = {8},
	Year = {2020}}

@inproceedings{vig_2020_causal,
	Author = {Jesse Vig and Sebastian Gehrmann and Yonatan Belinkov and Sharon Qian and Daniel Nevo and Yaron Singer and Stuart Shieber},
	Booktitle = {International Conference on Learning Representations},
	Title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
	Year = {2020}}

@inproceedings{sinitsin_2020_editable_neural,
	Author = {Anton Sinitsin and Vsevolod Plokhotnyuk and Dmitry Pyrkin and Sergei Popov and Artem Babenko},
	Booktitle = {International Conference on Machine Learning},
	Pages = {8946--8956},
	Title = {Editable Neural Networks},
	Year = {2020}}

@inproceedings{cao_2021_editing_factual_knowledge,
	Author = {Nicola De Cao and Wilker Aziz and Ivan Titov},
	Booktitle = {Conference on Empirical Methods in Natural Language Processing},
	Pages = {6491--6506},
	Title = {Editing Factual Knowledge in Language Models},
	Year = {2021}}

@inproceedings{goldstein_2022_locating,
	Author = {Dan Goldstein and Tatsunori B. Hashimoto and Christopher Potts},
	Booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
	Pages = {4948--4963},
	Title = {Locating and Editing Factual Associations in Marian},
	Year = {2022}}

@article{qiu_2020_pre_trained,
	Author = {Xipeng Qiu and Tianxiang Sun and Yige Xu and Yunfan Shao and Ning Dai and Xuanjing Huang},
	Journal = {AI Open},
	Pages = {26--37},
	Title = {Pre-trained Models for Natural Language Processing: A Survey},
	Volume = {1},
	Year = {2020}}

@article{dettmers_2024_qlora,
	Author = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
	Journal = {Advances in Neural Information Processing Systems},
	Title = {QLoRA: Efficient Finetuning of Quantized LLMs},
	Year = {2024}}

@inproceedings{liu_2024_dora,
	Author = {Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
	Booktitle = {International Conference on Machine Learning},
	Title = {DoRA: Weight-Decomposed Low-Rank Adaptation},
	Year = {2024}}

@article{zhang_2023_adalora,
	Author = {Qingru Zhang and Minshuo Chen and Alexander Bukharin and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
	Journal = {International Conference on Learning Representations},
	Title = {AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},
	Year = {2023}}

@article{wang_2023_knowledge_editing_survey,
	Author = {Yunzhi Yao and Peng Wang and Bozhong Tian and Siyuan Cheng and Zhoubo Li and Shumin Deng and Huajun Chen and Ningyu Zhang},
	Journal = {arXiv preprint arXiv:2310.16218},
	Title = {Editing Large Language Models: Problems, Methods, and Opportunities},
	Year = {2023}}

@inproceedings{yao_2023_easyedit,
	Author = {Yunzhi Yao and Peng Wang and Bozhong Tian and Siyuan Cheng and Zhoubo Li and Shumin Deng and Huajun Chen and Ningyu Zhang},
	Booktitle = {Annual Meeting of the Association for Computational Linguistics},
	Title = {EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models},
	Year = {2024}}

@article{dong_2022_survey_neural_editing,
	Author = {Qingxiu Dong and Jingjing Xu and Lingpeng Kong and Zhifang Sui and Lei Li},
	Journal = {arXiv preprint arXiv:2310.19704},
	Title = {A Survey on Knowledge Editing of Neural Networks},
	Year = {2023}}

@inproceedings{houlsby_2019_adapters,
	Author = {Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
	Booktitle = {International Conference on Machine Learning},
	Pages = {2790--2799},
	Title = {Parameter-Efficient Transfer Learning for NLP},
	Year = {2019}}

@article{li_2021_prefix_tuning,
	Author = {Xiang Lisa Li and Percy Liang},
	Journal = {Annual Meeting of the Association for Computational Linguistics},
	Title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
	Year = {2021}}

@article{lester_2021_prompt_tuning,
	Author = {Brian Lester and Rami Al-Rfou and Noah Constant},
	Journal = {Empirical Methods in Natural Language Processing},
	Title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
	Year = {2021}}


@article{zhong_2023_mquake,
	Author = {Zexuan Zhong and Zhengxuan Wu and Christopher D. Manning and Christopher Potts and Danqi Chen},
	Journal = {arXiv preprint arXiv:2305.14795},
	Title = {MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions},
	Year = {2023}}

@article{wang_2023_ripple_effects,
	Author = {Peng Wang and Ning Ding and Xinxi Lyu and Chunyan Miao},
	Journal = {Transactions of the Association for Computational Linguistics},
	Title = {Evaluating the Ripple Effects of Knowledge Editing in Language Models},
	Year = {2023}}

